{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#Using OpenAI Gym ToolKit for developing RL models #\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For practice: create an environment that already exists - CartPole-v1\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "env.observation_space #state space\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02912322,  0.0282733 , -0.00207801,  0.02744229])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The observation space is Box(4,) which is a 4-dimensional space\n",
    "# corresponding to 4 real-valued numbers:\n",
    "# 1) position of the car\n",
    "# 2) the cart's velocity\n",
    "# 3) the angle of the pole\n",
    "# 4) the velocity of the tip of the pole\n",
    "\n",
    "#The action space, Discrete(2), has two choices: move cart left or right\n",
    "\n",
    "#We can call .reset on the environment object, env, to reset to the pole's starting state (S0)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "#The values correspond to:\n",
    "#1) initital cart position; 2) velocity; 3) angle; 4) velocity of the pole\n",
    "# These values are initialized with random values with uniform distribution in the range [-0.05, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00761639,  0.00997466, -0.02257509,  0.00683202]), 1.0, False, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Can interact with the environment after reset by using the step() method\n",
    "\n",
    "env.reset()\n",
    "env.step(action = 0)\n",
    "env.step(action = 1)\n",
    "\n",
    "#The 1st action is to move to the left (action = 0) and the move to the right (action = 1)\n",
    "\n",
    "#The output is:\n",
    "# 1) an array for the new state (observations)\n",
    "# 2) A reward (scalar of type float)\n",
    "# 3) A termination flag (True or False)\n",
    "# 4) Dictionary with auxiliary information\n",
    "\n",
    "#The episode terminates (True) if the angle of the pole is larger than 12 degrees (+/-) \n",
    "# or the position of the car is more than 2.4 units from the center\n",
    "\n",
    "#The reward is to maximize the time the cart and pole are stabilized within the valid\n",
    "# regions - i.e the length of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# A Grid World Example #\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New environment, Grid World, a 4 (rows) x 6 (column) environment\n",
    "\n",
    "#For these examples - better to use a script rather than executing the code so switching to a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Implimenting a deep Q-Learning algorithm #\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the CartPole environment - implimenting deep Q-learning using hidden layers\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'reward',\n",
    "                  'next_state', 'done'))\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "            self, env, discount_factor = 0.95,\n",
    "            epsilon_greedy = 1.0, epsilon_min = 0.01, \n",
    "            epsilon_decay = 0.995, learning_rate = 1e-3,\n",
    "            max_memory_size = 2000):\n",
    "        self.enf = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "    \n",
    "        self.memory = deque(maxlen = max_memory_size)\n",
    "    \n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon_greedy\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.lr = learning_rate\n",
    "        self._build_nn_model()\n",
    "    \n",
    "    def _build_nn_model(self, n_layers = 3):\n",
    "        self.model = tf.keras.Sequential()\n",
    "\n",
    "        ##Hidden Layers\n",
    "        \n",
    "        for n in range(n_layers - 1):\n",
    "            self.model.add(tf.keras.layers.Dense(\n",
    "            units = 32, activation = 'relu'))\n",
    "\n",
    "            self.model.add(tf.keras.layers.Dense(\n",
    "            units = 32, activation = 'relu'))\n",
    "            \n",
    "        ## Last Layer\n",
    "        \n",
    "        self.model.add(tf.keras.layers.Dense(\n",
    "            units = self.action_size))\n",
    "        \n",
    "        #Build & compile model\n",
    "        self.model.build(input_shape = (None, self.state_size))\n",
    "        self.model.compile(\n",
    "            loss = 'mse',\n",
    "            optimizer = tf.keras.optimizers.Adam(lr = self.lr))\n",
    "        \n",
    "    def remember(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        q_values = self.model.predict(state)[0]\n",
    "        \n",
    "        return np.argmax(q_values) # returns action\n",
    "    \n",
    "    def _learn(self, batch_samples):\n",
    "        batch_states, batch_targets = [], []\n",
    "        for transition in batch_samples:\n",
    "            s, a, r, next_s, done = transition\n",
    "            if done:\n",
    "                target = r\n",
    "            else:\n",
    "                target = (r + \n",
    "                         self.gamma * np.argmax(\n",
    "                             self.model.predict(next_s)[0]))\n",
    "            target_all = self.model.predict(s)[0]\n",
    "            target_all[a] = target\n",
    "            batch_states.append(s.flatten())\n",
    "            batch_targets.append(target_all)\n",
    "            self._adjust_epsilon()\n",
    "        return self.model.fit(x = np.array(batch_states),\n",
    "                             y = np.array(batch_targets),\n",
    "                             epochs = 1,\n",
    "                             verbose = 0)\n",
    "    \n",
    "    def _adjust_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def replay(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        history = self._learn(samples)\n",
    "        return history.history['loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 0/200, Total rewards: 29\n",
      "Episodes: 1/200, Total rewards: 10\n",
      "Episodes: 2/200, Total rewards: 7\n",
      "Episodes: 3/200, Total rewards: 9\n",
      "Episodes: 4/200, Total rewards: 8\n",
      "Episodes: 5/200, Total rewards: 9\n",
      "Episodes: 6/200, Total rewards: 18\n",
      "Episodes: 7/200, Total rewards: 9\n",
      "Episodes: 8/200, Total rewards: 27\n",
      "Episodes: 9/200, Total rewards: 8\n",
      "Episodes: 10/200, Total rewards: 10\n",
      "Episodes: 11/200, Total rewards: 8\n",
      "Episodes: 12/200, Total rewards: 10\n",
      "Episodes: 13/200, Total rewards: 7\n",
      "Episodes: 14/200, Total rewards: 8\n",
      "Episodes: 15/200, Total rewards: 9\n",
      "Episodes: 16/200, Total rewards: 8\n",
      "Episodes: 17/200, Total rewards: 23\n",
      "Episodes: 18/200, Total rewards: 10\n",
      "Episodes: 19/200, Total rewards: 9\n",
      "Episodes: 20/200, Total rewards: 9\n",
      "Episodes: 21/200, Total rewards: 10\n",
      "Episodes: 22/200, Total rewards: 22\n",
      "Episodes: 23/200, Total rewards: 9\n",
      "Episodes: 24/200, Total rewards: 9\n",
      "Episodes: 25/200, Total rewards: 7\n",
      "Episodes: 26/200, Total rewards: 9\n",
      "Episodes: 27/200, Total rewards: 8\n",
      "Episodes: 28/200, Total rewards: 8\n",
      "Episodes: 29/200, Total rewards: 9\n",
      "Episodes: 30/200, Total rewards: 8\n",
      "Episodes: 31/200, Total rewards: 8\n",
      "Episodes: 32/200, Total rewards: 8\n",
      "Episodes: 33/200, Total rewards: 9\n",
      "Episodes: 34/200, Total rewards: 8\n",
      "Episodes: 35/200, Total rewards: 8\n",
      "Episodes: 36/200, Total rewards: 7\n",
      "Episodes: 37/200, Total rewards: 11\n",
      "Episodes: 38/200, Total rewards: 10\n",
      "Episodes: 39/200, Total rewards: 7\n",
      "Episodes: 40/200, Total rewards: 9\n",
      "Episodes: 41/200, Total rewards: 7\n",
      "Episodes: 42/200, Total rewards: 8\n",
      "Episodes: 43/200, Total rewards: 8\n",
      "Episodes: 44/200, Total rewards: 8\n",
      "Episodes: 45/200, Total rewards: 9\n",
      "Episodes: 46/200, Total rewards: 8\n",
      "Episodes: 47/200, Total rewards: 10\n",
      "Episodes: 48/200, Total rewards: 9\n",
      "Episodes: 49/200, Total rewards: 12\n",
      "Episodes: 50/200, Total rewards: 96\n",
      "Episodes: 51/200, Total rewards: 8\n",
      "Episodes: 52/200, Total rewards: 9\n",
      "Episodes: 53/200, Total rewards: 9\n",
      "Episodes: 54/200, Total rewards: 12\n",
      "Episodes: 55/200, Total rewards: 13\n",
      "Episodes: 56/200, Total rewards: 10\n",
      "Episodes: 57/200, Total rewards: 8\n",
      "Episodes: 58/200, Total rewards: 9\n",
      "Episodes: 59/200, Total rewards: 9\n",
      "Episodes: 60/200, Total rewards: 8\n",
      "Episodes: 61/200, Total rewards: 8\n",
      "Episodes: 62/200, Total rewards: 8\n",
      "Episodes: 63/200, Total rewards: 7\n",
      "Episodes: 64/200, Total rewards: 18\n",
      "Episodes: 65/200, Total rewards: 9\n",
      "Episodes: 66/200, Total rewards: 7\n",
      "Episodes: 67/200, Total rewards: 8\n",
      "Episodes: 68/200, Total rewards: 9\n",
      "Episodes: 69/200, Total rewards: 16\n",
      "Episodes: 70/200, Total rewards: 9\n",
      "Episodes: 71/200, Total rewards: 13\n",
      "Episodes: 72/200, Total rewards: 10\n",
      "Episodes: 73/200, Total rewards: 7\n",
      "Episodes: 74/200, Total rewards: 8\n",
      "Episodes: 75/200, Total rewards: 9\n",
      "Episodes: 76/200, Total rewards: 8\n",
      "Episodes: 77/200, Total rewards: 10\n",
      "Episodes: 78/200, Total rewards: 8\n",
      "Episodes: 79/200, Total rewards: 9\n",
      "Episodes: 80/200, Total rewards: 11\n",
      "Episodes: 81/200, Total rewards: 8\n",
      "Episodes: 82/200, Total rewards: 11\n",
      "Episodes: 83/200, Total rewards: 9\n",
      "Episodes: 84/200, Total rewards: 9\n",
      "Episodes: 85/200, Total rewards: 7\n",
      "Episodes: 86/200, Total rewards: 9\n",
      "Episodes: 87/200, Total rewards: 8\n",
      "Episodes: 88/200, Total rewards: 9\n",
      "Episodes: 89/200, Total rewards: 10\n",
      "Episodes: 90/200, Total rewards: 7\n",
      "Episodes: 91/200, Total rewards: 10\n",
      "Episodes: 92/200, Total rewards: 28\n",
      "Episodes: 93/200, Total rewards: 42\n",
      "Episodes: 94/200, Total rewards: 13\n",
      "Episodes: 95/200, Total rewards: 8\n",
      "Episodes: 96/200, Total rewards: 13\n",
      "Episodes: 97/200, Total rewards: 9\n",
      "Episodes: 98/200, Total rewards: 7\n",
      "Episodes: 99/200, Total rewards: 9\n",
      "Episodes: 100/200, Total rewards: 9\n",
      "Episodes: 101/200, Total rewards: 9\n",
      "Episodes: 102/200, Total rewards: 8\n",
      "Episodes: 103/200, Total rewards: 9\n",
      "Episodes: 104/200, Total rewards: 10\n",
      "Episodes: 105/200, Total rewards: 9\n",
      "Episodes: 106/200, Total rewards: 8\n",
      "Episodes: 107/200, Total rewards: 9\n",
      "Episodes: 108/200, Total rewards: 26\n",
      "Episodes: 109/200, Total rewards: 9\n",
      "Episodes: 110/200, Total rewards: 12\n",
      "Episodes: 111/200, Total rewards: 11\n",
      "Episodes: 112/200, Total rewards: 10\n",
      "Episodes: 113/200, Total rewards: 9\n",
      "Episodes: 114/200, Total rewards: 8\n",
      "Episodes: 115/200, Total rewards: 9\n",
      "Episodes: 116/200, Total rewards: 8\n",
      "Episodes: 117/200, Total rewards: 8\n",
      "Episodes: 118/200, Total rewards: 7\n",
      "Episodes: 119/200, Total rewards: 9\n",
      "Episodes: 120/200, Total rewards: 10\n",
      "Episodes: 121/200, Total rewards: 10\n",
      "Episodes: 122/200, Total rewards: 7\n",
      "Episodes: 123/200, Total rewards: 9\n",
      "Episodes: 124/200, Total rewards: 7\n",
      "Episodes: 125/200, Total rewards: 33\n",
      "Episodes: 126/200, Total rewards: 8\n",
      "Episodes: 127/200, Total rewards: 9\n",
      "Episodes: 128/200, Total rewards: 8\n",
      "Episodes: 129/200, Total rewards: 7\n",
      "Episodes: 130/200, Total rewards: 9\n",
      "Episodes: 131/200, Total rewards: 8\n",
      "Episodes: 132/200, Total rewards: 15\n",
      "Episodes: 133/200, Total rewards: 7\n",
      "Episodes: 134/200, Total rewards: 9\n",
      "Episodes: 135/200, Total rewards: 9\n",
      "Episodes: 136/200, Total rewards: 9\n",
      "Episodes: 137/200, Total rewards: 8\n",
      "Episodes: 138/200, Total rewards: 9\n",
      "Episodes: 139/200, Total rewards: 11\n",
      "Episodes: 140/200, Total rewards: 8\n",
      "Episodes: 141/200, Total rewards: 11\n",
      "Episodes: 142/200, Total rewards: 25\n",
      "Episodes: 143/200, Total rewards: 25\n",
      "Episodes: 144/200, Total rewards: 7\n",
      "Episodes: 145/200, Total rewards: 9\n",
      "Episodes: 146/200, Total rewards: 9\n",
      "Episodes: 147/200, Total rewards: 9\n",
      "Episodes: 148/200, Total rewards: 9\n",
      "Episodes: 149/200, Total rewards: 11\n",
      "Episodes: 150/200, Total rewards: 9\n",
      "Episodes: 151/200, Total rewards: 8\n",
      "Episodes: 152/200, Total rewards: 8\n",
      "Episodes: 153/200, Total rewards: 24\n",
      "Episodes: 154/200, Total rewards: 16\n",
      "Episodes: 155/200, Total rewards: 18\n",
      "Episodes: 156/200, Total rewards: 8\n",
      "Episodes: 157/200, Total rewards: 9\n",
      "Episodes: 158/200, Total rewards: 10\n",
      "Episodes: 159/200, Total rewards: 8\n",
      "Episodes: 160/200, Total rewards: 10\n",
      "Episodes: 161/200, Total rewards: 8\n",
      "Episodes: 162/200, Total rewards: 9\n",
      "Episodes: 163/200, Total rewards: 27\n",
      "Episodes: 164/200, Total rewards: 7\n",
      "Episodes: 165/200, Total rewards: 8\n",
      "Episodes: 166/200, Total rewards: 8\n",
      "Episodes: 167/200, Total rewards: 9\n",
      "Episodes: 168/200, Total rewards: 8\n",
      "Episodes: 169/200, Total rewards: 13\n",
      "Episodes: 170/200, Total rewards: 11\n",
      "Episodes: 171/200, Total rewards: 8\n",
      "Episodes: 172/200, Total rewards: 9\n",
      "Episodes: 173/200, Total rewards: 10\n",
      "Episodes: 174/200, Total rewards: 8\n",
      "Episodes: 175/200, Total rewards: 8\n",
      "Episodes: 176/200, Total rewards: 10\n",
      "Episodes: 177/200, Total rewards: 9\n",
      "Episodes: 178/200, Total rewards: 9\n",
      "Episodes: 179/200, Total rewards: 24\n",
      "Episodes: 180/200, Total rewards: 8\n",
      "Episodes: 181/200, Total rewards: 9\n",
      "Episodes: 182/200, Total rewards: 9\n",
      "Episodes: 183/200, Total rewards: 7\n",
      "Episodes: 184/200, Total rewards: 10\n",
      "Episodes: 185/200, Total rewards: 20\n",
      "Episodes: 186/200, Total rewards: 12\n",
      "Episodes: 187/200, Total rewards: 12\n",
      "Episodes: 188/200, Total rewards: 11\n",
      "Episodes: 189/200, Total rewards: 14\n",
      "Episodes: 190/200, Total rewards: 9\n",
      "Episodes: 191/200, Total rewards: 8\n",
      "Episodes: 192/200, Total rewards: 9\n",
      "Episodes: 193/200, Total rewards: 8\n",
      "Episodes: 194/200, Total rewards: 10\n",
      "Episodes: 195/200, Total rewards: 12\n",
      "Episodes: 196/200, Total rewards: 8\n",
      "Episodes: 197/200, Total rewards: 11\n",
      "Episodes: 198/200, Total rewards: 9\n",
      "Episodes: 199/200, Total rewards: 18\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8944c007aabd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mplot_learning_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8944c007aabd>\u001b[0m in \u001b[0;36mplot_learning_history\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     plt.plot(episodes, history[0], lw = 4,\n\u001b[1;32m      6\u001b[0m             marker = 'o', markersize = 10)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_history(history):\n",
    "    fig = plt.figure(1, figsize = (14, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    episodes = np.arange(len(history[0])) + 1\n",
    "    plt.plot(episodes, history[0], lw = 4,\n",
    "            marker = 'o', markersize = 10)\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = 15)\n",
    "    plt.xlabel('Episodes', size = 20)\n",
    "    plt.ylabel('# Total Rewards', size = 20)\n",
    "    plt.show()\n",
    "\n",
    "## General Settings\n",
    "\n",
    "EPISODES = 200\n",
    "batch_size = 32\n",
    "init_replay_memory_size = 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v1')\n",
    "    agent = DQNAgent(env)\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, agent.state_size])\n",
    "    \n",
    "    ## Filling up the replay-memory\n",
    "    for i in range(init_replay_memory_size):\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "        agent.remember(Transition(state, action, reward, \n",
    "                                  next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, agent.state_size])\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "    total_rewards, losses = [], []\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        if e % 10 == 0:\n",
    "            env.render()\n",
    "        state = np.reshape(state, [1, agent.state_size])\n",
    "        for i in range(500):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state,\n",
    "                                   [1, agent.state_size])\n",
    "            agent.remember(Transition(state, action, reward,\n",
    "                                     next_state, done))\n",
    "            state = next_state\n",
    "            if e % 10 == 0:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                total_rewards.append(i)\n",
    "                print('Episodes: %d/%d, Total rewards: %d'\n",
    "                     % (e, EPISODES, i))\n",
    "                \n",
    "                break\n",
    "            \n",
    "            loss = agent.replay(batch_size)\n",
    "            losses.append(loss)\n",
    "    plot_learning_history(total_rewards)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
