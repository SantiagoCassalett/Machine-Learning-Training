{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1112350\n",
      "Unique Characters: 80\n"
     ]
    }
   ],
   "source": [
    "#The goal is to develop a model that can generate new text that is similar in style\n",
    "# to the input document\n",
    "\n",
    "\n",
    "#Breaks the text down, character by character - uses memory of previous character to predict the next one\n",
    "\n",
    "#Three main steps:\n",
    "# 1) Prepare the data\n",
    "# 2) Build the RNN model\n",
    "# 3) Perform next-character prediction\n",
    "\n",
    "#Step 1A) Preparing the data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "##reading and processing text\n",
    "\n",
    "with open('1268-0.txt', 'r') as fp:\n",
    "    text = fp.read()\n",
    "    \n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text) #unique characters\n",
    "\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape: (1112350,)\n",
      "THE MYSTERIOUS  == Encoding ==> [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28] == Reverse ==> ISLAND\n"
     ]
    }
   ],
   "source": [
    "# Step 1B) Mapping characters to integers using a dictonary\n",
    "# NN and RNN cannot work with strings so must convert characters to integers \n",
    "# However, must reverse this process to get the results in text\n",
    "\n",
    "#Create two different functions - char2int and char_array\n",
    "\n",
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)} # mapper converting string to numeric\n",
    "\n",
    "char_array = np.array(chars_sorted) # reverse mapper\n",
    "\n",
    "text_encoded = np.array(\n",
    "    [char2int[ch] for ch in text],\n",
    "    dtype = np.int32) # contains the encoded values for all the characters\n",
    "\n",
    "print('Text encoded shape:', text_encoded.shape)\n",
    "\n",
    "print(text[:15], '== Encoding ==>', text_encoded[:15])\n",
    "print(text_encoded[15:21], '== Reverse ==>', ''.join(char_array[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 -> T\n",
      "32 -> H\n",
      "29 -> E\n",
      "1 ->  \n",
      "37 -> M\n"
     ]
    }
   ],
   "source": [
    "# Step 1C) Creating a TF dataset from the array, text_encoded\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
    "\n",
    "for ex in ds_text_encoded.take(5):\n",
    "    print('{} -> {}'.format(ex.numpy(), char_array[ex.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input (x):  'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
      " Target (y):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "\n",
      " Input (x):  ' Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n'\n",
      " Target (y):  'Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n\\n'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1D) Splitting the text into chunks using batch()\n",
    "\n",
    "# The hyperparameter for sequence length will be set at 40 char(\"sweet spot\") \n",
    "# The inputs x and targer y are offset by 1 so batches will be 41 char. \n",
    "# We will then apply a transformation using the map() method to separate the x and the y sequences\n",
    "\n",
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "ds_chunks = ds_text_encoded.batch(chunk_size, drop_remainder=True) # drops all that dont fit the 41 batch size\n",
    "\n",
    "##define the function for splitting x & y\n",
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]\n",
    "    target_seq = chunk[1:]\n",
    "    return input_seq, target_seq\n",
    "\n",
    "ds_sequences = ds_chunks.map(split_input_target)\n",
    "\n",
    "#Example sequences from the transformed dataset\n",
    "\n",
    "for example in ds_sequences.take(2):\n",
    "    print(' Input (x): ',\n",
    "         repr(''.join(char_array[example[0].numpy()])))\n",
    "    print(' Target (y): ',\n",
    "         repr(''.join(char_array[example[1].numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1E) Divide the dataset into mini-batches\n",
    "#Shuffle the training examples and divide the inputs into mini-batches - each batch will contain\n",
    "# multiple training examples - multiple sentences\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "ds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 256)         20480     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 512)         1574912   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 80)          41040     \n",
      "=================================================================\n",
      "Total params: 1,636,432\n",
      "Trainable params: 1,636,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 2) Building a Character-Level RNN Model\n",
    "# For reusability - we will build a function, build_model() that defines an RNN model\n",
    "# using the Keras Sequential class\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units,\n",
    "            return_sequences = True),\n",
    "        tf.keras.layers.Dense(vocab_size) #activation = None - need logits\n",
    "    ])    \n",
    "    return model\n",
    "\n",
    "## Setting the training parameters\n",
    "charset_size = len(char_array)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "model = build_model(\n",
    "    vocab_size = charset_size,\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "424/424 [==============================] - 248s 585ms/step - loss: 2.3205\n",
      "Epoch 2/20\n",
      "424/424 [==============================] - 297s 701ms/step - loss: 1.7480\n",
      "Epoch 3/20\n",
      "424/424 [==============================] - 285s 672ms/step - loss: 1.5464\n",
      "Epoch 4/20\n",
      "424/424 [==============================] - 268s 633ms/step - loss: 1.4306\n",
      "Epoch 5/20\n",
      "424/424 [==============================] - 264s 622ms/step - loss: 1.3560\n",
      "Epoch 6/20\n",
      "424/424 [==============================] - 255s 601ms/step - loss: 1.3039\n",
      "Epoch 7/20\n",
      "424/424 [==============================] - 242s 571ms/step - loss: 1.2658\n",
      "Epoch 8/20\n",
      "424/424 [==============================] - 262s 618ms/step - loss: 1.2342\n",
      "Epoch 9/20\n",
      "424/424 [==============================] - 271s 639ms/step - loss: 1.2087\n",
      "Epoch 10/20\n",
      "424/424 [==============================] - 286s 674ms/step - loss: 1.1869\n",
      "Epoch 11/20\n",
      "424/424 [==============================] - 311s 733ms/step - loss: 1.1674\n",
      "Epoch 12/20\n",
      "424/424 [==============================] - 329s 775ms/step - loss: 1.1501\n",
      "Epoch 13/20\n",
      "424/424 [==============================] - 342s 806ms/step - loss: 1.1340\n",
      "Epoch 14/20\n",
      "424/424 [==============================] - 284s 670ms/step - loss: 1.1181\n",
      "Epoch 15/20\n",
      "424/424 [==============================] - 290s 684ms/step - loss: 1.1037\n",
      "Epoch 16/20\n",
      "424/424 [==============================] - 289s 681ms/step - loss: 1.0907\n",
      "Epoch 17/20\n",
      "424/424 [==============================] - 314s 740ms/step - loss: 1.0775\n",
      "Epoch 18/20\n",
      "424/424 [==============================] - 265s 625ms/step - loss: 1.0645\n",
      "Epoch 19/20\n",
      "424/424 [==============================] - 239s 563ms/step - loss: 1.0519\n",
      "Epoch 20/20\n",
      "424/424 [==============================] - 290s 683ms/step - loss: 1.0394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff4d2c12990>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the above model\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits = True))\n",
    "\n",
    "model.fit(ds, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.33333334 0.33333334 0.33333334]\n",
      "array([[0, 0, 1, 2, 0, 0, 0, 0, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Evaluating phase - generating new text passages\n",
    "\n",
    "# Can use the softmax function to regularize the logit output into probabilities\n",
    "# Need to randomly sample from the outputs though instead of just chosing the element with the maximum value\n",
    "\n",
    "# An example is below:\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "logits = [[1.0, 1.0, 1.0]] # equiprobable categories - same logits for the 3 categories [0, 1, 2]\n",
    "\n",
    "print('Probabilities:', tf.math.softmax(logits).numpy()[0]) # equal probablities for each class per logits\n",
    "\n",
    "samples = tf.random.categorical(\n",
    "    logits = logits, num_samples = 10)\n",
    "\n",
    "tf.print(samples.numpy())\n",
    "\n",
    "#Would expect with infinite sample size - occurances would be 1/3 for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.10650698 0.10650698 0.78698605]\n",
      "array([[2, 0, 2, 2, 2, 0, 1, 2, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "#Changing the logits to favor the third category\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "logits = [[1.0, 1.0, 3.0]]\n",
    "\n",
    "print('Probabilities:', tf.math.softmax(logits).numpy()[0]) #Expect more samples to be drawn from category 2\n",
    "\n",
    "samples = tf.random.categorical(\n",
    "    logits = logits, num_samples = 10)\n",
    "\n",
    "tf.print(samples.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can use the tf.random.categorical() function to generate samples based on the logits from the model\n",
    "# We can define a function, sample() that:\n",
    "    #1) Receives a short starting string, starting_str\n",
    "    #2) Generates a new string, generated_str. \n",
    "    #3) String of max_input_length is then taken from the end of generated_string and encoded\n",
    "    # to a sequence of integers, encoded_input\n",
    "    #4) Encoded_input is passed to the RNN to compute the logits\n",
    "    #5) Last element of the output logits is passed to tf.random.categorical to generate a new sample\n",
    "    #6) This is converted to a character then appended to generated string, generated_text\n",
    "    #7) Repeat the above process until reaching a string of desire length\n",
    "# Note that the output from the RNN is a sequence of logits with the same length as the input sequence\n",
    "# since we specified return_sequence=True\n",
    "# Each element in the output represents the logits (vector of size 80) for the next character after\n",
    "# observing the input sequence by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the last element of the output logits (O^(t)) which is passed to the tf.random.categorical()\n",
    "# to generate a new sample.\n",
    "# The new sample is converted to a character, which is then appended to the end of the generate string\n",
    "# generated_text, increasing its length by 1.\n",
    "# The process is repeated taking the last max_input_length number of characters from the end of the \n",
    "# generated_str, and using that to generate a new character until the length of the string reaches the desire value\n",
    "# This process is refered to as auto-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str, \n",
    "          len_generated_text = 500,\n",
    "          max_input_length = 40,\n",
    "          scale_factor = 1.0):\n",
    "\n",
    "    encoded_input = [char2int[s] for s in starting_str]\n",
    "    encoded_input = tf.reshape(encoded_input, (1, -1))\n",
    "    \n",
    "    generated_str = starting_str\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(len_generated_text):\n",
    "        logits = model(encoded_input) # model with the new input\n",
    "        logits = tf.squeeze(logits, 0)\n",
    "        \n",
    "        scaled_logits = logits * scale_factor\n",
    "        new_char_indx = tf.random.categorical(\n",
    "            scaled_logits, num_samples=1)\n",
    "        \n",
    "        new_char_indx = tf.squeeze(new_char_indx)[-1].numpy()\n",
    "        \n",
    "        generated_str += str(char_array[new_char_indx])\n",
    "        \n",
    "        new_char_indx = tf.expand_dims([new_char_indx], 0)\n",
    "        encoded_input = tf.concat(\n",
    "            [encoded_input, new_char_indx],\n",
    "            axis=1)\n",
    "        \n",
    "        encoded_input = encoded_input[:, -max_input_length:]\n",
    "        \n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Island is not deceived. The tide were not certain? It had landed on, he must listen, mingled with the cart soil, not even seemed as if he would have been irrows and point of discovery.\n",
      "\n",
      "All first necessary for several days to Port Balloon. They halted seen in some determined even top of a fine sension of fact? It appeared sail now to reach\n",
      "the voyage\n",
      "violence to roughly, without any sounday.\n",
      "\n",
      "The settlers had not believe that the rocks for heard. We will celled, it was composed to sail in basadiars. B\n"
     ]
    }
   ],
   "source": [
    "#generating new text\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "print(sample(model, starting_str='The Island'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities before scaling:       [0.10650698 0.10650698 0.78698604]\n",
      "Probabilities after scaling with 0.5:     [0.21194156 0.21194156 0.57611688]\n",
      "Probabilities after scaling with 0.1:      [0.31042377 0.31042377 0.37915245]\n"
     ]
    }
   ],
   "source": [
    "#You can alter how the text is generated to make it be less random and follow the \n",
    "# learned text patterns better\n",
    "# This altered via the scaling factor, alpha (in the model it was 1.0), < 1 is more predictable and >1 is less\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "logits = np.array([[1.0, 1.0, 3.0]])\n",
    "\n",
    "print('Probabilities before scaling:      ',\n",
    "     tf.math.softmax(logits).numpy()[0])\n",
    "\n",
    "print('Probabilities after scaling with 0.5:    ',\n",
    "     tf.math.softmax(logits * 0.5).numpy()[0])\n",
    "\n",
    "print('Probabilities after scaling with 0.1:     ',\n",
    "     tf.math.softmax(logits * 0.1).numpy()[0])\n",
    "\n",
    "#By scaling with <1 - the probabilties computed by softmax become more uniform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
